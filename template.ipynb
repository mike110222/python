{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import os\n",
    "import numpy as np\n",
    "import sympy as sp\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "dspath=\"./datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def dummyf(x):\n",
    "\treturn x**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "for i in range(decoded.shape[0]):\n",
    "\n",
    "    array=decoded[i,0,:,:,0]\n",
    "    \n",
    "    # Plot the array as a grayscale image\n",
    "    plt.imshow(array, cmap='gray')\n",
    "\n",
    "    plt.colorbar()  # Add a color bar to the side for reference\n",
    "    plt.title('Grayscale Image of 128x128 Array')\n",
    "    plt.show()\n",
    "\"\"\"\n",
    "\n",
    "fig, axes = plt.subplots(10, 6, figsize=(12, 15))\n",
    "for i in range(10):\n",
    "\tfor j in range(6):\n",
    "\t\tarray=decoded[6*i+j,0,:,:,0]\n",
    "\t\taxes[i, j].imshow(array, cmap='gray')\n",
    "\t\taxes[i, j].set_title(f\"Plot {6*i+j+1}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# jax checkpoints\n",
    "import jax.numpy as jnp\n",
    "from flax.training import checkpoints\n",
    "import os\n",
    "ckpt_dir = os.getcwd()+\"/checkpoints/TEST\"\n",
    "\n",
    "restored={}\n",
    "state = {'a':jnp.ones(16)}\n",
    "#restored = checkpoints.restore_checkpoint(ckpt_dir, state, prefix=f\"_ARRAY_\")\n",
    "\n",
    "checkpoints.save_checkpoint(ckpt_dir, state, 1, prefix=f\"_ARRAY_\", keep=5, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic TrainState\n",
    "import flax.linen as nn\n",
    "from flax.training.train_state import TrainState\n",
    "import jax, jax.numpy as jnp\n",
    "import optax\n",
    "\n",
    "x = jnp.ones((1,2))\n",
    "y = jnp.ones((1,2))\n",
    "z = jnp.array([[1,1],[-2,4],[0,3]])\n",
    "model = nn.Dense(2)\n",
    "variables = model.init(jax.random.key(0), x)\n",
    "tx = optax.adam(2e-2)\n",
    "\n",
    "state=TrainState.create(apply_fn=model.apply, params=variables, tx=tx)\n",
    "\n",
    "#print(state.params['params']['kernel'])\n",
    "#print(model.apply(state.params,z))\n",
    "\n",
    "def loss(params, x, y):\n",
    "\tpredictions=state.apply_fn(params,x)\n",
    "\treturn optax.l2_loss(predictions,y).mean()\n",
    "print(loss(state.params,x,y))\n",
    "\n",
    "for _ in range(300):\n",
    "\t\n",
    "\tgrads=jax.grad(loss)(state.params,x,y)\n",
    "\tstate=state.apply_gradients(grads=grads)\n",
    "\t#display(loss(state.params,x,y))\n",
    "\n",
    "print(state.params)\n",
    "print(loss(state.params,x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformers pipeline\n",
    "# long loading time\n",
    "from transformers import pipeline\n",
    "\n",
    "'''# Test a pre-trained pipeline (e.g., sentiment-analysis)\n",
    "classifier = pipeline(task='sentiment-analysis',device='cuda')\n",
    "print(classifier(\"Hello World\"))\n",
    "print(classifier([\"Hugging Face is amazing!\",\"Red Blue yellow\"]))\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\", device='cuda')\n",
    "print(generator(\"Once upon a time\", max_length=50, num_return_sequences=1))\n",
    "print(generator(\"I've always been shy growing up.\", max_length=250, num_return_sequences=1))'''\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokens1 = tokenizer(\"Hello, how are you?\", return_tensors=\"pt\")\n",
    "tokens2 = tokenizer(\"Hello\", return_tensors=\"pt\")\n",
    "print(tokens1)\n",
    "print(tokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch BatchNorm evaluation mode issue\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Define a BatchNorm layer\n",
    "bn = nn.BatchNorm1d(3,momentum=0.1)\n",
    "\n",
    "# Simulate training mode\n",
    "bn.train()  # Training mode\n",
    "input_train = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]])\n",
    "output_train = bn(input_train)\n",
    "print(\"Training Mode Output:\", output_train)\n",
    "\n",
    "'''mean_rows = np.mean(np.array(input_train), axis=0)\n",
    "\n",
    "# Variance along columns (axis=0)\n",
    "variance_cols = np.var(np.array(input_train), axis=0)\n",
    "\n",
    "print(\"Mean along rows:\", mean_rows)\n",
    "print(\"Variance along columns:\", variance_cols)'''\n",
    "\n",
    "\n",
    "'''print([pn, p] for pn,p in bn.named_parameters())\n",
    "for pn,p in bn.named_parameters():\n",
    "\tprint([pn,p])'''\n",
    "print(bn.state_dict())\n",
    "\n",
    "# Switch to evaluation mode\n",
    "bn.eval()  # Evaluation mode\n",
    "input_eval = torch.tensor([[4.0, 1.0, 6.0],[7.0, 8.0, 9.0]])\n",
    "output_eval = bn(input_eval)\n",
    "print(\"Evaluation Mode Output:\", output_eval)\n",
    "print(bn.state_dict())\n",
    "\n",
    "input_eval = torch.tensor([[7.0, 8.0, 9.0]])\n",
    "output_eval = bn(input_eval)\n",
    "print(\"Evaluation Mode Output:\", output_eval)\n",
    "print(bn.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.2879, grad_fn=<SinBackward0>)\n",
      "tensor(16., grad_fn=<PowBackward0>)\n",
      "tensor(4., requires_grad=True)\n",
      "tensor(-7.6613)\n",
      "tensor(-0.9577)\n",
      "None\n",
      "tensor([0.0000, 0.7638, 0.0000, 0.0000, 1.4254], grad_fn=<ReluBackward0>)\n",
      "tensor([0., 1., 0., 0., 1.])\n",
      "[tensor(1.), tensor(2.), None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_746837/1281635405.py:23: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
      "  print(z.grad)\n",
      "/tmp/ipykernel_746837/1281635405.py:49: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
      "  print( [i.grad for i in iter([a1,a2,a3])] )\n"
     ]
    }
   ],
   "source": [
    "# torch .backward, checkpoint\n",
    "from typing import Iterable\n",
    "from networkx import non_randomness\n",
    "from sqlalchemy import false\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "\n",
    "x = torch.tensor(4.0, requires_grad=True)\n",
    "y = x ** 2  # y = x²\n",
    "y.retain_grad()\n",
    "z = torch.sin(y)\n",
    "\n",
    "print(z)\n",
    "print(y)\n",
    "print(x)\n",
    "\n",
    "z.backward()  # Compute gradients\n",
    "print(x.grad)  # Output: 4.0 (dy/dx = 2*x, at x=2)\n",
    "print(y.grad)\n",
    "print(z.grad)\n",
    "\n",
    "# Define a function to be checkpointed\n",
    "def forward_func(input):\n",
    "    return torch.relu(input)\n",
    "#pdb.set_trace()\n",
    "\n",
    "# Input tensor\n",
    "input_tensor = torch.randn(5, requires_grad=True)\n",
    "\n",
    "# Use checkpoint\n",
    "output = checkpoint(forward_func, input_tensor, use_reentrant=False)\n",
    "\n",
    "print(output)\n",
    "\n",
    "# Backward pass\n",
    "z=output.sum()\n",
    "z.backward()\n",
    "print(input_tensor.grad)\n",
    "\n",
    "a1=torch.tensor(2.0, requires_grad=True)\n",
    "a2=torch.tensor(1.0, requires_grad=True)\n",
    "#a3=torch.tensor([a1+2*a2, a1*a2])\n",
    "a3=a1+2*a2\n",
    "a3.backward(gradient=torch.tensor(1))\n",
    "#a3.backward()\n",
    "print( [i.grad for i in iter([a1,a2,a3])] )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @wrap decorator\n",
    "from functools import wraps\n",
    "\n",
    "def my_decorator(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        print(\"Wrapper called\")\n",
    "        return func(*args, **kwargs)\n",
    "    return wrapper\n",
    "\n",
    "@my_decorator\n",
    "def my_function():\n",
    "    \"\"\"This is my function.\"\"\"\n",
    "    pass\n",
    "\n",
    "print(my_function.__doc__) \n",
    "my_function()\n",
    "#my_decorator(my_function)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZhipuAI video content description\n",
    "from zhipuai import ZhipuAI\n",
    "\n",
    "client = ZhipuAI(api_key='c40bdf30c49ef1d2f40a98b10af2df84.qibzbV4iQBseorFg') # Fill in your own APIKey\n",
    "\n",
    "'''response = client.chat.completions.create(\n",
    "    model=\"glm-4v-plus\",  # Fill in the model name to be called\n",
    "    messages=[\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "          {\n",
    "            \"type\": \"video_url\",\n",
    "            \"video_url\": {\n",
    "                \"url\" : \"https://videos.pexels.com/video-files/3195394/3195394-sd_426_240_25fps.mp4\"\n",
    "            }\n",
    "          },\n",
    "          {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": \"Please describe this video in detail\"\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "    ]\n",
    ")'''\n",
    "\n",
    "'''response = client.chat.completions.create(\n",
    "    model=\"glm-4-plus\",  # Fill in the model code you need to call\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an assistant who loves to answer various questions, your task is to provide users with professional, accurate, and insightful advice.\"},\n",
    "        {\"role\": \"user\", \"content\": \"A farmer needs to take a wolf, a sheep, and cabbage across a river, but can only take one item at a time, and the wolf and sheep cannot be left alone, nor can the sheep and cabbage. How should the farmer cross the river?\"}\n",
    "    ],\n",
    ")'''\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"glm-4-plus\",  # Fill in the model code you need to call\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"\"},\n",
    "        {\"role\": \"user\", \"content\": \"Evaluate the integral ∫x²dx\"}\n",
    "    ],\n",
    ")\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all-MiniLM-L6-v2, embedding, 23M params, usage;\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "# Sentences we want sentence embeddings for\n",
    "sentences = ['This is an example sentence', 'Each sentence is converted']\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "# Perform pooling\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "# Normalize embeddings\n",
    "sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "print(\"Sentence embeddings:\")\n",
    "print(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight must be cloned\n",
    "n, d, m = 3, 5, 7\n",
    "embedding = nn.Embedding(n, d, max_norm=1.0)\n",
    "W = torch.randn((m, d), requires_grad=True)\n",
    "idx = torch.tensor([1, 2])\n",
    "a = embedding.weight.clone() @ W.t()  # weight must be cloned for this to be differentiable\n",
    "b = embedding(idx) @ W.t()  # modifies weight in-place\n",
    "out = (a.unsqueeze(0) + b.unsqueeze(1))\n",
    "loss = out.sigmoid().prod()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# window calculator\n",
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "\n",
    "def calculate_sum():\n",
    "    try:\n",
    "        num1 = float(entry1.get())\n",
    "        num2 = float(entry2.get())\n",
    "        result = num1 + num2\n",
    "        messagebox.showinfo(\"Result\", f\"The sum is: {result}\")\n",
    "    except ValueError:\n",
    "        messagebox.showerror(\"Error\", \"Please enter valid numbers\")\n",
    "\n",
    "# Create the main window\n",
    "window = tk.Tk()\n",
    "window.title(\"Sum Calculator\")\n",
    "\n",
    "# Create and place the labels and entry widgets\n",
    "tk.Label(window, text=\"Enter the first number:\").grid(row=0, column=0)\n",
    "entry1 = tk.Entry(window)\n",
    "entry1.grid(row=0, column=1)\n",
    "\n",
    "tk.Label(window, text=\"Enter the second number:\").grid(row=1, column=0)\n",
    "entry2 = tk.Entry(window)\n",
    "entry2.grid(row=1, column=1)\n",
    "\n",
    "# Create and place the calculate button\n",
    "button = tk.Button(window, text=\"Calculate\", command=calculate_sum)\n",
    "button.grid(row=2, columnspan=2)\n",
    "\n",
    "# Start the GUI event loop\n",
    "window.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env02",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
