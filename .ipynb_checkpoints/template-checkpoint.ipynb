{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import os\n",
    "import numpy as np\n",
    "import sympy as sp\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "dspath=\"./datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def dummyf(x):\n",
    "\treturn x**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "for i in range(decoded.shape[0]):\n",
    "\n",
    "    array=decoded[i,0,:,:,0]\n",
    "    \n",
    "    # Plot the array as a grayscale image\n",
    "    plt.imshow(array, cmap='gray')\n",
    "\n",
    "    plt.colorbar()  # Add a color bar to the side for reference\n",
    "    plt.title('Grayscale Image of 128x128 Array')\n",
    "    plt.show()\n",
    "\"\"\"\n",
    "\n",
    "fig, axes = plt.subplots(10, 6, figsize=(12, 15))\n",
    "for i in range(10):\n",
    "\tfor j in range(6):\n",
    "\t\tarray=decoded[6*i+j,0,:,:,0]\n",
    "\t\taxes[i, j].imshow(array, cmap='gray')\n",
    "\t\taxes[i, j].set_title(f\"Plot {6*i+j+1}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# jax checkpoints\n",
    "import jax.numpy as jnp\n",
    "from flax.training import checkpoints\n",
    "import os\n",
    "ckpt_dir = os.getcwd()+\"/checkpoints/TEST\"\n",
    "\n",
    "restored={}\n",
    "state = {'a':jnp.ones(16)}\n",
    "#restored = checkpoints.restore_checkpoint(ckpt_dir, state, prefix=f\"_ARRAY_\")\n",
    "\n",
    "checkpoints.save_checkpoint(ckpt_dir, state, 1, prefix=f\"_ARRAY_\", keep=5, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic TrainState\n",
    "import flax.linen as nn\n",
    "from flax.training.train_state import TrainState\n",
    "import jax, jax.numpy as jnp\n",
    "import optax\n",
    "\n",
    "x = jnp.ones((1,2))\n",
    "y = jnp.ones((1,2))\n",
    "z = jnp.array([[1,1],[-2,4],[0,3]])\n",
    "model = nn.Dense(2)\n",
    "variables = model.init(jax.random.key(0), x)\n",
    "tx = optax.adam(2e-2)\n",
    "\n",
    "state=TrainState.create(apply_fn=model.apply, params=variables, tx=tx)\n",
    "\n",
    "#print(state.params['params']['kernel'])\n",
    "#print(model.apply(state.params,z))\n",
    "\n",
    "def loss(params, x, y):\n",
    "\tpredictions=state.apply_fn(params,x)\n",
    "\treturn optax.l2_loss(predictions,y).mean()\n",
    "print(loss(state.params,x,y))\n",
    "\n",
    "for _ in range(300):\n",
    "\t\n",
    "\tgrads=jax.grad(loss)(state.params,x,y)\n",
    "\tstate=state.apply_gradients(grads=grads)\n",
    "\t#display(loss(state.params,x,y))\n",
    "\n",
    "print(state.params)\n",
    "print(loss(state.params,x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformers pipeline\n",
    "# long loading time\n",
    "from transformers import pipeline\n",
    "\n",
    "'''# Test a pre-trained pipeline (e.g., sentiment-analysis)\n",
    "classifier = pipeline(task='sentiment-analysis',device='cuda')\n",
    "print(classifier(\"Hello World\"))\n",
    "print(classifier([\"Hugging Face is amazing!\",\"Red Blue yellow\"]))\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\", device='cuda')\n",
    "print(generator(\"Once upon a time\", max_length=50, num_return_sequences=1))\n",
    "print(generator(\"I've always been shy growing up.\", max_length=250, num_return_sequences=1))'''\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokens1 = tokenizer(\"Hello, how are you?\", return_tensors=\"pt\")\n",
    "tokens2 = tokenizer(\"Hello\", return_tensors=\"pt\")\n",
    "print(tokens1)\n",
    "print(tokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch BatchNorm evaluation mode issue\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Define a BatchNorm layer\n",
    "bn = nn.BatchNorm1d(3,momentum=0.1)\n",
    "\n",
    "# Simulate training mode\n",
    "bn.train()  # Training mode\n",
    "input_train = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]])\n",
    "output_train = bn(input_train)\n",
    "print(\"Training Mode Output:\", output_train)\n",
    "\n",
    "'''mean_rows = np.mean(np.array(input_train), axis=0)\n",
    "\n",
    "# Variance along columns (axis=0)\n",
    "variance_cols = np.var(np.array(input_train), axis=0)\n",
    "\n",
    "print(\"Mean along rows:\", mean_rows)\n",
    "print(\"Variance along columns:\", variance_cols)'''\n",
    "\n",
    "\n",
    "'''print([pn, p] for pn,p in bn.named_parameters())\n",
    "for pn,p in bn.named_parameters():\n",
    "\tprint([pn,p])'''\n",
    "print(bn.state_dict())\n",
    "\n",
    "# Switch to evaluation mode\n",
    "bn.eval()  # Evaluation mode\n",
    "input_eval = torch.tensor([[4.0, 1.0, 6.0],[7.0, 8.0, 9.0]])\n",
    "output_eval = bn(input_eval)\n",
    "print(\"Evaluation Mode Output:\", output_eval)\n",
    "print(bn.state_dict())\n",
    "\n",
    "input_eval = torch.tensor([[7.0, 8.0, 9.0]])\n",
    "output_eval = bn(input_eval)\n",
    "print(\"Evaluation Mode Output:\", output_eval)\n",
    "print(bn.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.2879, grad_fn=<SinBackward0>)\n",
      "tensor(16., grad_fn=<PowBackward0>)\n",
      "tensor(4., requires_grad=True)\n",
      "tensor(-7.6613)\n",
      "tensor(-0.9577)\n",
      "None\n",
      "tensor([0.0000, 0.7638, 0.0000, 0.0000, 1.4254], grad_fn=<ReluBackward0>)\n",
      "tensor([0., 1., 0., 0., 1.])\n",
      "[tensor(1.), tensor(2.), None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_746837/1281635405.py:23: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
      "  print(z.grad)\n",
      "/tmp/ipykernel_746837/1281635405.py:49: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
      "  print( [i.grad for i in iter([a1,a2,a3])] )\n"
     ]
    }
   ],
   "source": [
    "# torch .backward, checkpoint\n",
    "from typing import Iterable\n",
    "from networkx import non_randomness\n",
    "from sqlalchemy import false\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "\n",
    "x = torch.tensor(4.0, requires_grad=True)\n",
    "y = x ** 2  # y = x²\n",
    "y.retain_grad()\n",
    "z = torch.sin(y)\n",
    "\n",
    "print(z)\n",
    "print(y)\n",
    "print(x)\n",
    "\n",
    "z.backward()  # Compute gradients\n",
    "print(x.grad)  # Output: 4.0 (dy/dx = 2*x, at x=2)\n",
    "print(y.grad)\n",
    "print(z.grad)\n",
    "\n",
    "# Define a function to be checkpointed\n",
    "def forward_func(input):\n",
    "    return torch.relu(input)\n",
    "#pdb.set_trace()\n",
    "\n",
    "# Input tensor\n",
    "input_tensor = torch.randn(5, requires_grad=True)\n",
    "\n",
    "# Use checkpoint\n",
    "output = checkpoint(forward_func, input_tensor, use_reentrant=False)\n",
    "\n",
    "print(output)\n",
    "\n",
    "# Backward pass\n",
    "z=output.sum()\n",
    "z.backward()\n",
    "print(input_tensor.grad)\n",
    "\n",
    "a1=torch.tensor(2.0, requires_grad=True)\n",
    "a2=torch.tensor(1.0, requires_grad=True)\n",
    "#a3=torch.tensor([a1+2*a2, a1*a2])\n",
    "a3=a1+2*a2\n",
    "a3.backward(gradient=torch.tensor(1))\n",
    "#a3.backward()\n",
    "print( [i.grad for i in iter([a1,a2,a3])] )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @wrap decorator\n",
    "from functools import wraps\n",
    "\n",
    "def my_decorator(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        print(\"Wrapper called\")\n",
    "        return func(*args, **kwargs)\n",
    "    return wrapper\n",
    "\n",
    "@my_decorator\n",
    "def my_function():\n",
    "    \"\"\"This is my function.\"\"\"\n",
    "    pass\n",
    "\n",
    "print(my_function.__doc__) \n",
    "my_function()\n",
    "#my_decorator(my_function)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZhipuAI video content description\n",
    "from zhipuai import ZhipuAI\n",
    "\n",
    "client = ZhipuAI(api_key='c40bdf30c49ef1d2f40a98b10af2df84.qibzbV4iQBseorFg') # Fill in your own APIKey\n",
    "\n",
    "'''response = client.chat.completions.create(\n",
    "    model=\"glm-4v-plus\",  # Fill in the model name to be called\n",
    "    messages=[\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "          {\n",
    "            \"type\": \"video_url\",\n",
    "            \"video_url\": {\n",
    "                \"url\" : \"https://videos.pexels.com/video-files/3195394/3195394-sd_426_240_25fps.mp4\"\n",
    "            }\n",
    "          },\n",
    "          {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": \"Please describe this video in detail\"\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "    ]\n",
    ")'''\n",
    "\n",
    "'''response = client.chat.completions.create(\n",
    "    model=\"glm-4-plus\",  # Fill in the model code you need to call\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an assistant who loves to answer various questions, your task is to provide users with professional, accurate, and insightful advice.\"},\n",
    "        {\"role\": \"user\", \"content\": \"A farmer needs to take a wolf, a sheep, and cabbage across a river, but can only take one item at a time, and the wolf and sheep cannot be left alone, nor can the sheep and cabbage. How should the farmer cross the river?\"}\n",
    "    ],\n",
    ")'''\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"glm-4-plus\",  # Fill in the model code you need to call\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"\"},\n",
    "        {\"role\": \"user\", \"content\": \"Evaluate the integral ∫x²dx\"}\n",
    "    ],\n",
    ")\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all-MiniLM-L6-v2, embedding, 23M params, usage;\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "# Sentences we want sentence embeddings for\n",
    "sentences = ['This is an example sentence', 'Each sentence is converted']\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "# Perform pooling\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "# Normalize embeddings\n",
    "sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "print(\"Sentence embeddings:\")\n",
    "print(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight must be cloned\n",
    "n, d, m = 3, 5, 7\n",
    "embedding = nn.Embedding(n, d, max_norm=1.0)\n",
    "W = torch.randn((m, d), requires_grad=True)\n",
    "idx = torch.tensor([1, 2])\n",
    "a = embedding.weight.clone() @ W.t()  # weight must be cloned for this to be differentiable\n",
    "b = embedding(idx) @ W.t()  # modifies weight in-place\n",
    "out = (a.unsqueeze(0) + b.unsqueeze(1))\n",
    "loss = out.sigmoid().prod()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# window calculator\n",
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "\n",
    "def calculate_sum():\n",
    "    try:\n",
    "        num1 = float(entry1.get())\n",
    "        num2 = float(entry2.get())\n",
    "        result = num1 + num2\n",
    "        messagebox.showinfo(\"Result\", f\"The sum is: {result}\")\n",
    "    except ValueError:\n",
    "        messagebox.showerror(\"Error\", \"Please enter valid numbers\")\n",
    "\n",
    "# Create the main window\n",
    "window = tk.Tk()\n",
    "window.title(\"Sum Calculator\")\n",
    "\n",
    "# Create and place the labels and entry widgets\n",
    "tk.Label(window, text=\"Enter the first number:\").grid(row=0, column=0)\n",
    "entry1 = tk.Entry(window)\n",
    "entry1.grid(row=0, column=1)\n",
    "\n",
    "tk.Label(window, text=\"Enter the second number:\").grid(row=1, column=0)\n",
    "entry2 = tk.Entry(window)\n",
    "entry2.grid(row=1, column=1)\n",
    "\n",
    "# Create and place the calculate button\n",
    "button = tk.Button(window, text=\"Calculate\", command=calculate_sum)\n",
    "button.grid(row=2, columnspan=2)\n",
    "\n",
    "# Start the GUI event loop\n",
    "window.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sklearn example\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Step 1: Scale features\n",
    "    ('classifier', RandomForestClassifier(random_state=42))  # Step 2: Classifier\n",
    "])\n",
    "\n",
    "# Set hyperparameters for grid search\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [50, 100, 200],\n",
    "    'classifier__max_depth': [None, 10, 20],\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample flax, optax\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "\n",
    "# Define a simple model using Flax\n",
    "class MLP(nn.Module):\n",
    "    features: int\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(self.features)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(1)(x)\n",
    "        return x\n",
    "\n",
    "# Define the loss function\n",
    "def loss_fn(params, model, batch):\n",
    "    inputs, targets = batch\n",
    "    predictions = model.apply({'params': params}, inputs)\n",
    "    return jnp.mean((predictions - targets) ** 2)\n",
    "\n",
    "# Training step\n",
    "@jax.jit\n",
    "def train_step(state, batch):\n",
    "    grad_fn = jax.value_and_grad(loss_fn)\n",
    "    loss, grads = grad_fn(state.params, state.apply_fn, batch)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, loss\n",
    "\n",
    "# Initialize model, optimizer, and training state\n",
    "model = MLP(features=128)\n",
    "rng = jax.random.PRNGKey(0)\n",
    "x = jnp.ones((1, 10))  # Example input\n",
    "params = model.init(rng, x)['params']\n",
    "\n",
    "optimizer = optax.adam(learning_rate=0.001)\n",
    "state = train_state.TrainState.create(\n",
    "    apply_fn=model.apply,\n",
    "    params=params,\n",
    "    tx=optimizer\n",
    ")\n",
    "\n",
    "# Example batch\n",
    "batch = (jnp.ones((32, 10)), jnp.ones((32, 1)))\n",
    "\n",
    "# Training loop\n",
    "for _ in range(100):\n",
    "    state, loss = train_step(state, batch)\n",
    "    print(f\"Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines with a: 88, lines with b: 50\n"
     ]
    }
   ],
   "source": [
    "# Apache Spark\n",
    "\"\"\"SimpleApp.py\"\"\"\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "logFile = \"/home/mike_1102/miniGPT/minGPT/README.md\"  # Should be some file on your system\n",
    "spark = SparkSession.builder.appName(\"SimpleApp\").getOrCreate()\n",
    "logData = spark.read.text(logFile).cache()\n",
    "\n",
    "numAs = logData.filter(logData.value.contains('a')).count()\n",
    "numBs = logData.filter(logData.value.contains('b')).count()\n",
    "\n",
    "print(\"Lines with a: %i, lines with b: %i\" % (numAs, numBs))\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/30 22:10:42 ERROR ArrowPythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/mike_1102/miniconda3/envs/env02/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1225, in main\n",
      "    eval_type = read_int(infile)\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mike_1102/miniconda3/envs/env02/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.lang.UnsupportedOperationException: sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available\n",
      "\tat org.apache.arrow.memory.util.MemoryUtil.directBuffer(MemoryUtil.java:174)\n",
      "\tat org.apache.arrow.memory.ArrowBuf.getDirectBuffer(ArrowBuf.java:229)\n",
      "\tat org.apache.arrow.memory.ArrowBuf.nioBuffer(ArrowBuf.java:224)\n",
      "\tat org.apache.arrow.vector.ipc.WriteChannel.write(WriteChannel.java:133)\n",
      "\tat org.apache.arrow.vector.ipc.message.MessageSerializer.writeBatchBuffers(MessageSerializer.java:303)\n",
      "\tat org.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:276)\n",
      "\tat org.apache.arrow.vector.ipc.ArrowWriter.writeRecordBatch(ArrowWriter.java:147)\n",
      "\tat org.apache.arrow.vector.ipc.ArrowWriter.writeBatch(ArrowWriter.java:133)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream(PythonArrowInput.scala:140)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream$(PythonArrowInput.scala:124)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowPythonRunner.writeIteratorToArrowStream(ArrowPythonRunner.scala:30)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.$anonfun$writeIteratorToStream$1(PythonArrowInput.scala:96)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.writeIteratorToStream(PythonArrowInput.scala:102)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "24/11/30 22:10:42 ERROR ArrowPythonRunner: This may have been caused by a prior exception:\n",
      "java.lang.UnsupportedOperationException: sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available\n",
      "\tat org.apache.arrow.memory.util.MemoryUtil.directBuffer(MemoryUtil.java:174)\n",
      "\tat org.apache.arrow.memory.ArrowBuf.getDirectBuffer(ArrowBuf.java:229)\n",
      "\tat org.apache.arrow.memory.ArrowBuf.nioBuffer(ArrowBuf.java:224)\n",
      "\tat org.apache.arrow.vector.ipc.WriteChannel.write(WriteChannel.java:133)\n",
      "\tat org.apache.arrow.vector.ipc.message.MessageSerializer.writeBatchBuffers(MessageSerializer.java:303)\n",
      "\tat org.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:276)\n",
      "\tat org.apache.arrow.vector.ipc.ArrowWriter.writeRecordBatch(ArrowWriter.java:147)\n",
      "\tat org.apache.arrow.vector.ipc.ArrowWriter.writeBatch(ArrowWriter.java:133)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream(PythonArrowInput.scala:140)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream$(PythonArrowInput.scala:124)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowPythonRunner.writeIteratorToArrowStream(ArrowPythonRunner.scala:30)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.$anonfun$writeIteratorToStream$1(PythonArrowInput.scala:96)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.writeIteratorToStream(PythonArrowInput.scala:102)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "24/11/30 22:10:42 ERROR Executor: Exception in task 0.0 in stage 23.0 (TID 135)\n",
      "java.lang.UnsupportedOperationException: sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available\n",
      "\tat org.apache.arrow.memory.util.MemoryUtil.directBuffer(MemoryUtil.java:174)\n",
      "\tat org.apache.arrow.memory.ArrowBuf.getDirectBuffer(ArrowBuf.java:229)\n",
      "\tat org.apache.arrow.memory.ArrowBuf.nioBuffer(ArrowBuf.java:224)\n",
      "\tat org.apache.arrow.vector.ipc.WriteChannel.write(WriteChannel.java:133)\n",
      "\tat org.apache.arrow.vector.ipc.message.MessageSerializer.writeBatchBuffers(MessageSerializer.java:303)\n",
      "\tat org.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:276)\n",
      "\tat org.apache.arrow.vector.ipc.ArrowWriter.writeRecordBatch(ArrowWriter.java:147)\n",
      "\tat org.apache.arrow.vector.ipc.ArrowWriter.writeBatch(ArrowWriter.java:133)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream(PythonArrowInput.scala:140)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream$(PythonArrowInput.scala:124)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowPythonRunner.writeIteratorToArrowStream(ArrowPythonRunner.scala:30)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.$anonfun$writeIteratorToStream$1(PythonArrowInput.scala:96)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.writeIteratorToStream(PythonArrowInput.scala:102)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "24/11/30 22:10:42 WARN TaskSetManager: Lost task 0.0 in stage 23.0 (TID 135) (10.255.255.254 executor driver): java.lang.UnsupportedOperationException: sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available\n",
      "\tat org.apache.arrow.memory.util.MemoryUtil.directBuffer(MemoryUtil.java:174)\n",
      "\tat org.apache.arrow.memory.ArrowBuf.getDirectBuffer(ArrowBuf.java:229)\n",
      "\tat org.apache.arrow.memory.ArrowBuf.nioBuffer(ArrowBuf.java:224)\n",
      "\tat org.apache.arrow.vector.ipc.WriteChannel.write(WriteChannel.java:133)\n",
      "\tat org.apache.arrow.vector.ipc.message.MessageSerializer.writeBatchBuffers(MessageSerializer.java:303)\n",
      "\tat org.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:276)\n",
      "\tat org.apache.arrow.vector.ipc.ArrowWriter.writeRecordBatch(ArrowWriter.java:147)\n",
      "\tat org.apache.arrow.vector.ipc.ArrowWriter.writeBatch(ArrowWriter.java:133)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream(PythonArrowInput.scala:140)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream$(PythonArrowInput.scala:124)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowPythonRunner.writeIteratorToArrowStream(ArrowPythonRunner.scala:30)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.$anonfun$writeIteratorToStream$1(PythonArrowInput.scala:96)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.writeIteratorToStream(PythonArrowInput.scala:102)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "\n",
      "24/11/30 22:10:42 ERROR TaskSetManager: Task 0 in stage 23.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39magg(mean_udf(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv\u001b[39m\u001b[38;5;124m'\u001b[39m]))\u001b[38;5;241m.\u001b[39mcollect())\n\u001b[1;32m     16\u001b[0m spark \u001b[38;5;241m=\u001b[39mSparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSimpleApp\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[0;32m---> 17\u001b[0m main(spark)\n",
      "Cell \u001b[0;32mIn[12], line 14\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(spark)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;129m@pandas_udf\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdouble\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean_udf\u001b[39m(v: pd\u001b[38;5;241m.\u001b[39mSeries) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m v\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39magg(mean_udf(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv\u001b[39m\u001b[38;5;124m'\u001b[39m]))\u001b[38;5;241m.\u001b[39mcollect())\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1395\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1373\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/env02/lib/python3.11/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/env02/lib/python3.11/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def main(spark):\n",
    "    df = spark.createDataFrame(\n",
    "        [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n",
    "        (\"id\", \"v\"))\n",
    "\n",
    "    @pandas_udf(\"double\")\n",
    "    def mean_udf(v: pd.Series) -> float:\n",
    "        return v.mean()\n",
    "\n",
    "    print(df.groupby(\"id\").agg(mean_udf(df['v'])).collect())\n",
    "\n",
    "spark =SparkSession.builder.appName(\"SimpleApp\").getOrCreate()\n",
    "main(spark)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env02",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
